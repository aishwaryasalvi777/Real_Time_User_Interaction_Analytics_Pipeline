ğŸ›°ï¸ Real-Time User Interaction Analytics Pipeline
This project simulates a real-time e-commerce environment where user interactions (like views, add-to-cart, and purchases) are streamed, aggregated, and analyzed to detect churn and visualize funnel trends using an end-to-end data pipeline.

ğŸ¯ Objective
Build a full-stack real-time analytics system that:

Ingests simulated user activity via Kafka.

Processes streams with Spark Structured Streaming.

Writes aggregates and predictions to PostgreSQL.

Predicts user churn in real time using a trained model.

Visualizes data with an interactive Streamlit dashboard.

ğŸ“Œ Key Features
âœ… Kafka-based real-time event ingestion.

âœ… Spark Streaming for session-level aggregation.

âœ… Churn prediction using Logistic Regression.

âœ… PostgreSQL for structured storage of KPIs and churn.

âœ… Streamlit dashboard for real-time funnel and churn analysis.

âœ… Modular & Dockerized for easy deployment.

ğŸ§± Architecture
mermaid
Copy
Edit
flowchart TD
    A[Kafka Producer: Simulated Events] --> B[Spark Streaming]
    B --> C1[Funnel Aggregates â†’ PostgreSQL]
    B --> C2[Churn Features â†’ Logistic Regression Model]
    C2 --> C3[Churn Predictions â†’ PostgreSQL]
    C1 & C3 --> D[Streamlit Dashboard]
ğŸ“‚ Project Structure
pgsql
Copy
Edit
realtime-user-analytics/
â”œâ”€â”€ kafka_producer/
â”‚   â””â”€â”€ producer.py
â”œâ”€â”€ spark_streaming/
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ aggregator.py
â”‚   â””â”€â”€ churn_feature_engineering.py
â”œâ”€â”€ postgres_writer/
â”‚   â””â”€â”€ writer.py
â”œâ”€â”€ ml_model/
â”‚   â”œâ”€â”€ train_churn_model.py
â”‚   â”œâ”€â”€ churn_model.pkl
â”‚   â””â”€â”€ fastapi_app.py
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ kafka_config.json
â”‚   â”œâ”€â”€ spark_config.json
â”‚   â””â”€â”€ db_config.json
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_all.sh
â””â”€â”€ README.md
âš™ï¸ Tech Stack
Layer	Tech
Event Stream	Apache Kafka
Processing	Apache Spark Structured Streaming
Storage	PostgreSQL
Model	Scikit-learn Logistic Regression
Dashboard	Streamlit
Deployment	Docker, Docker Compose
API	FastAPI (for model deployment, optional)

ğŸš€ How to Run
Pre-requisites: Docker, Python 3.10+, pip

1. Clone Repo
bash
Copy
Edit
git clone https://github.com/your-username/realtime-user-analytics.git
cd realtime-user-analytics
2. Start Docker Services
bash
Copy
Edit
docker-compose -f docker/docker-compose.yml up
3. Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
4. Train Churn Model
bash
Copy
Edit
python ml_model/train_churn_model.py
5. Run Kafka Producer
bash
Copy
Edit
python kafka_producer/producer.py
6. Start Spark Streaming Jobs
bash
Copy
Edit
python spark_streaming/consumer.py
python spark_streaming/aggregator.py
python spark_streaming/churn_feature_engineering.py
7. Launch Dashboard
bash
Copy
Edit
streamlit run dashboard/app.py
ğŸ“Š Dashboard Features
Tab	Description
ğŸ“ˆ Real-Time KPIs	View, Add-to-Cart, Purchase trends by time
ğŸ”„ Funnel Summary	Conversion funnel per session/user
âŒ› User-Level Stats	Per-user event tracking
ğŸ›‘ At-Risk Users	Real-time churn prediction using ML

Dashboard auto-refreshes every 10s.

ğŸ§  Model Info: Churn Prediction
Features: num_views, num_adds_to_cart

Target: Session that ends without purchase â†’ churn

Model: Logistic Regression (balanced, interpretable)

Format: Trained using train_churn_model.py, saved as churn_model.pkl

ğŸ§ª Testing the System
Kafka emits biased events: 60% views, 30% add-to-cart, 10% purchases.

PostgreSQL stores tables:

user_events_raw

funnel_summary

churn_predictions

ğŸ” Interview Soundbites
â€œBuilt an end-to-end real-time data pipeline using Kafka â†’ Spark â†’ PostgreSQL â†’ Streamlit.â€

â€œPredicted churn based on session-level features using a deployed Logistic Regression model.â€

â€œDesigned fully containerized environment with modular ETL + analytics pipeline.â€

â€œDashboard auto-refreshes to show live KPIs and churn insights for business stakeholders.â€

ğŸ“Œ Future Improvements
Add user segmentation & cohort analysis.

Integrate email/SMS alerts for churned users.

Use PyTorch or XGBoost for better churn prediction.

Expose FastAPI endpoint for real-time scoring from external tools.

