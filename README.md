Real-Time User Interaction Analytics Pipeline

ðŸ§  Objective:

Build a real-time data pipeline to simulate, stream, process, store, and visualize user behavior events (views, cart additions, purchases) from an e-commerce platform. The goal is to enable funnel analysis, churn prediction, and live KPIs to support marketing and product decisions.

âš™ï¸ Architecture Overview:

Tech Stack: Kafka, Spark Structured Streaming, PostgreSQL, FastAPI, Streamlit, Docker Compose

+-------------+        +----------------+         +----------------+         +-------------------+        +-------------+
| Kafka       | -----> | Spark Consumer | ----->  | Spark Aggregator| -----> | PostgreSQL        | -----> | Streamlit   |
| Producer    |        | (Raw Events)   |         | (Funnel Counts)|         | (Event DB + KPIs) |        | Dashboard   |
+-------------+        +----------------+         +----------------+         +-------------------+        +-------------+
                                                                       \
                                                                        \-> FastAPI (Churn Model Serving)

ðŸ“¦ Project Structure

realtime-user-analytics/
â”œâ”€â”€ kafka_producer/           # Simulates real-time user behavior
â”‚   â””â”€â”€ producer.py
â”‚
â”œâ”€â”€ spark_streaming/          # Real-time data processing using Spark
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ aggregator.py         # Performs windowed aggregations for funnel KPIs
â”‚   â””â”€â”€ churn_feature_engineering.py  # Generates churn features & writes to PostgreSQL
â”‚
â”œâ”€â”€ postgres_writer/          # Writes aggregated outputs to PostgreSQL
â”‚   â””â”€â”€ writer.py
â”‚
â”œâ”€â”€ ml_model/                 # Churn model + API
â”‚   â”œâ”€â”€ train_churn_model.py
â”‚   â”œâ”€â”€ fastapi_app.py
â”‚   â””â”€â”€ churn_model.pkl
â”‚
â”œâ”€â”€ dashboard/                # Streamlit dashboard
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ config/                   # Configs for Kafka, Spark, and DB
â”‚
â”œâ”€â”€ docker/                   # Docker Compose for Kafka, PostgreSQL, Spark
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_all.sh                # Convenience script to launch the pipeline
â””â”€â”€ README.md

ðŸ”„ Phase-by-Phase Breakdown

âœ… Phase 1: Kafka Producer Setup

Implemented producer.py to emit synthetic user events (view, add_to_cart, purchase) with realistic bias.

Ensured timestamp is included for proper event-time tracking.

Topics sent to Kafka: user_events

âœ… Phase 2: Spark Consumer and Aggregator

consumer.py reads Kafka messages and writes raw events to PostgreSQL (raw_events table).

aggregator.py performs 10-minute tumbling window aggregations with watermarking.

Results stored in funnel_summary for dashboard usage.

âœ… Phase 3: Feature Engineering for Churn

Created churn_feature_engineering.py to read events and build churn-related features like:

Total view/add_to_cart/purchase events per user

Time since last interaction

Spark watermarking used with correctness-safe logic to avoid late event issues.

Writes features into churn_features table in PostgreSQL.

âœ… Phase 4: Churn Prediction Model

Built model using logistic regression in train_churn_model.py.

Trained on aggregated behavioral data.

Model saved as churn_model.pkl.

fastapi_app.py exposes /predict endpoint to serve real-time churn predictions.

âœ… Phase 5: Real-Time Dashboard

Streamlit dashboard in dashboard/app.py:

Funnel KPIs (view â†’ cart â†’ purchase)

Live churn predictions (churned / not churned users)

Auto-refresh for real-time visibility

âœ… Deliverables:

Real-time Kafka â†’ Spark â†’ PostgreSQL â†’ Dashboard pipeline

Docker Compose setup for reproducibility

Trained churn prediction model & live prediction API

Interactive Streamlit dashboard

ðŸ§ª How to Run

Start the full pipeline:

bash run_all.sh

Open the Streamlit dashboard:

http://localhost:8501

Trigger churn predictions (optional):

Exposed via FastAPI

curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" -d '{"user_id": "user_123"}'

ðŸš€ Key Highlights for Interviews

Spark Structured Streaming: windowed aggregations + watermarking for correctness

Model Deployment: FastAPI + real-time inference

Kafka and PostgreSQL integration

Streamlit dashboard for live KPI tracking

End-to-end Dockerized pipeline

âœ… Project Complete â€” Real-Time Behavioral Analytics & Churn Model

